{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Vs Numba Comparison\n",
    "- Implementation of a simple neural network using PyTorch and Numba jit\n",
    "- Speed calculation of both implementations\n",
    "- Surface plot of the loss function for both implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import functions as fn\n",
    "from math import sqrt\n",
    "import random\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch AutoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x, data_y = fn.gen_pts_(1000)\n",
    "xp, yp = 5.0, 5.0\n",
    "\n",
    "# fn.loss(xp, yp, data_x, data_y)\n",
    "#* define loss function in python\n",
    "def Lossfn(xp, yp, data_x, data_y):\n",
    "    loss = sum([((xi - xp)**2  + (yi - yp)**2)**0.5 for xi, yi in zip(data_x, data_y)]) / len(data_x)\n",
    "    return loss\n",
    "#* assurance that the function is correct\n",
    "print(fn.loss(xp, yp, data_x, data_y) == Lossfn(xp, yp, data_x, data_y))\n",
    "\n",
    "#* create tensors from data\n",
    "data = torch.tensor([data_x, data_y]).T\n",
    "pnt = torch.tensor([xp, yp], requires_grad=True)\n",
    "\n",
    "#* compute gradient numerically\n",
    "def GradLimit(xp, yp, data_x, data_y, H=0.001):\n",
    "    dl_dx = (Lossfn(xp + H, yp, data_x, data_y) - Lossfn(xp, yp, data_x, data_y)) / H\n",
    "    dl_dy = (Lossfn(xp, yp + H, data_x, data_y) - Lossfn(xp, yp, data_x, data_y)) / H\n",
    "\n",
    "    return dl_dx, dl_dy\n",
    "\n",
    "#* compute gradient analytically - close form\n",
    "def GradCloseform(xp, yp, data_x, data_y):\n",
    "    sum_x, sum_y = 0, 0\n",
    "    for xi, yi in zip(data_x, data_y):\n",
    "        inv_sqrt = ((xi - xp) ** 2 + (yi - yp) ** 2) ** -0.5\n",
    "        sum_x += inv_sqrt * (xi - xp)\n",
    "        sum_y += inv_sqrt * (yi - yp)\n",
    "\n",
    "    dl_dx = -sum_x / len(data_x)\n",
    "    dl_dy = -sum_y / len(data_y)\n",
    "    return dl_dx, dl_dy\n",
    "\n",
    "#* compute gradient auto-grad\n",
    "def GradTroch(data, pnt):\n",
    "    loss = torch.mean(torch.sqrt(torch.sum((data - pnt) ** 2, dim=1)))\n",
    "    loss.backward()\n",
    "    return pnt.grad\n",
    "\n",
    "\n",
    "print(GradLimit(xp, yp, data_x, data_y))\n",
    "print(GradCloseform(xp, yp, data_x, data_y))\n",
    "print(GradTroch(data, pnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_timer = timeit.timeit(lambda: GradLimit(xp, yp, data_x, data_y), number=100)\n",
    "closeform_timer = timeit.timeit(lambda: GradCloseform(xp, yp, data_x, data_y), number=100)\n",
    "torch_timer = timeit.timeit(lambda: GradTroch(data, pnt), number=100)\n",
    "\n",
    "print(f\"close form is faster than limit by {(limit_timer / closeform_timer):.3f} times\")\n",
    "print(f\"torch is faster than limit by {(limit_timer / torch_timer):.3f} times\")\n",
    "print(f\"torch is faster than close form by {(closeform_timer / torch_timer):.3f} times\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments \n",
    "- it's faster to use closed form than numerical limits\n",
    "- it's faster to use torch autograd than closed form"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba compilation \n",
    "- numba compiles python code to machine code with LLVM\n",
    "- numba is a just-in-time compiler\n",
    "- code \n",
    "  - first compiled to IR\n",
    "  - then executed\n",
    "\n",
    "- the first time of execution is slow as compilation is done\n",
    "- the second time of execution is fast as the compiled code is executed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss mesh \n",
    "- Compile loss mesh using normal python types, list, dict, etc\n",
    "- speed up with numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from numba import jit\n",
    "    import warnings\n",
    "    import time\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "except:\n",
    "    print(\n",
    "        \"\"\"\n",
    "          Numba is not installed.\n",
    "          Comment the JIT decorator and try again.\n",
    "            \"\"\"\n",
    "    )\n",
    "    \n",
    "\n",
    "@jit(nopython=True)\n",
    "def Lossfn(xp, yp, data_x, data_y):\n",
    "    loss = sum(\n",
    "        [((xi - xp) ** 2 + (yi - yp) ** 2) ** 0.5 for xi, yi in zip(data_x, data_y)]\n",
    "    ) / len(data_x)\n",
    "    return loss\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def CreatLossMesh(step, mesh_size, data_x, data_y):\n",
    "    STEP = step\n",
    "    MESH_SIZE = mesh_size\n",
    "    x_mesh = [-1 + i * STEP for i in range(MESH_SIZE)]\n",
    "    y_mesh = [-1 + i * STEP for i in range(MESH_SIZE)]\n",
    "    loss_mesh = [[Lossfn(x, y, data_x, data_y) for x in x_mesh] for y in y_mesh]\n",
    "    return loss_mesh\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "loss_mesh = CreatLossMesh(0.01, 200, data_x, data_y)\n",
    "end = time.time()\n",
    "sped_mesh_jit_1st = end - start\n",
    "\n",
    "start = time.time()\n",
    "loss_mesh = CreatLossMesh(0.01, 200, data_x, data_y)\n",
    "end = time.time()\n",
    "sped_mesh_jit_2nd = end - start\n",
    "\n",
    "print(f\"first run: {sped_mesh_jit_1st:.3f} seconds\")\n",
    "print(f\"second run: {sped_mesh_jit_2nd:.3f} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Loss Mesh\n",
    "- split loss mesh into two parts \n",
    "  - create mesh, define loss \n",
    "  - compute loss\n",
    "- comput all loss mesh in one go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_loss_fn = lambda pnt, data : torch.mean(torch.sqrt(torch.sum((data - pnt) ** 2, dim=1)))\n",
    "torch_data = torch.tensor([data_x, data_y]).T\n",
    "torch_pnt = torch.tensor([xp, yp], requires_grad=True)\n",
    "x_mesh = torch.linspace(-1, 2, 300)\n",
    "y_mesh = torch.linspace(-1, 2, 300)\n",
    "print(\"mesh size: \", len(x_mesh) * len(y_mesh))\n",
    "\n",
    "start = time.time()\n",
    "loss_mesh_tensor = [\n",
    "    [torch_loss_fn(torch.tensor([x,y]), torch_data) for x in x_mesh] for y in y_mesh\n",
    "]\n",
    "end = time.time()\n",
    "sped_mesh_torch_1st = end - start\n",
    "print(f\"first run: {sped_mesh_torch_1st:.3f} seconds\")\n",
    "#* Numba is faster xd? ofc we are still using using python interpreter for the computation\n",
    "#* anyway this is much faster than using the previous method without numba\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "# mp.style.use(plt.style.available[6])\n",
    "ax = fig.add_subplot(1, 1, 1, projection = '3d')\n",
    "#ax.stem() #TODO steam plot\n",
    "ax.contour(x_mesh, y_mesh, loss_mesh_tensor, levels =200)\n",
    "ax.view_init(elev=0,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d plot of single surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(np.array(x_mesh), np.array(y_mesh), np.array(loss_mesh_tensor), cmap='viridis', edgecolor='none')\n",
    "ax.view_init(elev=0,)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recompute the loss mesh with torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_data = torch.tensor([data_x, data_y]).t()\n",
    "x_space = torch.linspace(-1, 2, 300).repeat(1, 300).t() #* (90000, 1)\n",
    "y_space = torch.linspace(-1, 2, 300).repeat(300, 1).t().reshape(-1, 1) #* (90000, 1)\n",
    "print(x_space.shape, y_space.shape)\n",
    "grid_points = torch.hstack((x_space, y_space)) #* (90000, 2)\n",
    "print(grid_points.shape)\n",
    "\n",
    "def FasterGridLoss(pnts, data):\n",
    "    \n",
    "    data = data.repeat(pnts.shape[0], 1, 1) #* (90000, 1000, 2)\n",
    "    data = data.view(-1, pnts.shape[0], 2) #* (1000, 90000, 2)\n",
    "    #* apply the simple loss approach\n",
    "    \n",
    "    res = (data - pnts) ** 2\n",
    "    res = torch.sum(res, dim=2) ** 0.5 \n",
    "    loss = torch.mean(res, dim=0)\n",
    "    # loss = loss.view(300, 300)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "grid_faster = FasterGridLoss(grid_points, torch_data).tolist()\n",
    "adjusted_grid = [grid_faster[i::300] for i in range(300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = 0.01\n",
    "MESH_SIZE = 300\n",
    "\n",
    "#* define mesh [-1, 2] for x and y\n",
    "x_mesh = [-1 +i * STEP for i in range(MESH_SIZE)]\n",
    "y_mesh = [-1 +i * STEP for i in range(MESH_SIZE)]\n",
    "\n",
    "\n",
    "@jit(nopython=False, parallel=True)\n",
    "def plt_contour():\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    # mp.style.use(plt.style.available[6])\n",
    "    ax = fig.add_subplot(1, 1, 1, projection = '3d')\n",
    "    ax.contour(x_mesh, y_mesh, adjusted_grid, levels =100)\n",
    "    ax.view_init(elev=0,)\n",
    "    plt.show()\n",
    "    \n",
    "plt_contour()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_space = torch.linspace(-1, 2, 300)\n",
    "y_space = torch.linspace(-1, 2, 300)\n",
    "x, y = torch.meshgrid(x_space, y_space, indexing='xy')\n",
    "data_x, data_y = fn.gen_pts_(1000)\n",
    "data = torch.tensor([data_x, data_y]).t()\n",
    "pnts = torch.stack([x, y], dim=2).reshape(-1, 2)\n",
    "\n",
    "\n",
    "\n",
    "def FasterGridLoss(pnts, data):\n",
    "    print(pnts.shape, data.shape)\n",
    "    data = data.repeat(pnts.shape[0], 1, 1) #* (90000, 1000, 2)\n",
    "    data = data.view(-1, pnts.shape[0], 2) #* (1000, 90000, 2)\n",
    "    #* apply the simple loss approach\n",
    "    \n",
    "    res = (data - pnts) ** 2\n",
    "    res = torch.sum(res, dim=2) ** 0.5 \n",
    "    loss = torch.mean(res, dim=0)\n",
    "    # loss = loss.view(300, 300)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "grid_faster = FasterGridLoss(pnts, data).view(300, 300)\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x.numpy(), y.numpy(), grid_faster.numpy(), cmap='viridis', edgecolor='none')\n",
    "ax.view_init(elev=0,)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL TO UNDERSTAND"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repeat and hstack  \n",
    "![repeat](misc/repeat.png)\n",
    "```python\n",
    "x.repeat(2,1) #* repeat 2 times along dim 0, 1 times along dim 1\n",
    "```\n",
    "\n",
    "![stack](misc/stack.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axies = torch.linspace(0, 4, 5).repeat(1, 5).t()\n",
    "y_axies = torch.linspace(0, 4, 5).repeat(5, 1).t().reshape(-1, 1)\n",
    "print(x_axies.shape, y_axies.shape)\n",
    "pnts = torch.hstack((x_axies, y_axies))\n",
    "print(pnts.shape)\n",
    "\n",
    "plt.scatter(pnts[:, 0], pnts[:, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes \n",
    "- Goal is to create a Grid of points in 2d space\n",
    "- create grid with has 90000 points each with 2 coordinates\n",
    "- calculate loss for each point in the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_space = torch.linspace(-1, 2, 300).repeat(1, 300).t()\n",
    "y_space = torch.linspace(-1, 2, 300).repeat(300, 1).t().reshape(-1, 1)\n",
    "pnts = torch.hstack((x_space, y_space))\n",
    "# print(x_space.shape, y_space.shape), print(pnts.shape)\n",
    "# plt.scatter(pnts[:, 0], pnts[:, 1])\n",
    "#* TAKES SO MUCH MEMORY "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the faster formulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* data.shape = torch.Size([1000, 2])\n",
    "#* pnts.shape = torch.Size([90000, 2])\n",
    "data_repeated = data.repeat((pnts.shape[0], 1, 1)) #* torch.Size([90000, 1000, 2])\n",
    "#* repeat the the whole data 90000 times along dim 0\n",
    "(data_repeated[0] == data).all() #* True\n",
    "\n",
    "repeated_batches = data_repeated.view(-1, pnts.shape[0], 2) #* torch.Size([90000, 1000, 2])\n",
    "repeated_data_permuted = data_repeated.permute(1, 0, 2)  #* torch.Size([90000, 1000, 2])\n",
    "print(\"it's not premutation \", (repeated_data_permuted == repeated_batches).all()) #* True\n",
    "print(\"repeated grid for each point in the data \", (repeated_batches[0] == repeated_batches[1]).all())\n",
    "print(repeated_batches.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHy does this work ?\n",
    "- `.view(-1, 90000, 2)` will take the first 90000 elements and reshape them into a 90000x2 matrix\n",
    "- since elements are repeated each 1000x2, we get 90x1000x2 matrix\n",
    "- this ensure that the each 90000x2 matrix has the same elements\n",
    "- changing the data size or the grid size will not break the code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
