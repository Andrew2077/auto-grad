{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation (chain rule)\n",
    "- instead of calculating the gradient of the loss wr to each parameter, we can calculate the gradient of the loss wr to each parameter in the network using chain rule\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{i=0}^{N-1}\\sqrt{(x_i - x_p)^2 + (y_i - y_p)^2}\n",
    "$$\n",
    "\n",
    "### Chain rule -> split loss into small functions \n",
    "- define new parts, calculate loss wr to $X_p$, same steps can be applied to $\\frac{\\partial{L}}{\\partial{y_p}} $\n",
    "- caluclate the local gradients at each step\n",
    "  #### difference \n",
    "$$ I_x = x_i -x_p \\rightarrow \\frac{\\partial{I_x}}{\\partial{x_p}} = -1 $$\n",
    "\n",
    "$$ I_y = y_i -y_p \\rightarrow \\frac{\\partial{I_y}}{\\partial{x_p}} = 0 $$\n",
    "\n",
    "- #### squaring \n",
    "  \n",
    "$$ g_x = I_x^2 \\rightarrow \\frac{\\partial{g_x}}{\\partial{I_x}} = 2 * I_x $$ \n",
    "\n",
    "$$ g_y = I_y^2 \\rightarrow \\frac{\\partial{g_x}}{\\partial{I_y}} = 2 * I_y $$\n",
    "\n",
    "- #### sum\n",
    "  \n",
    "$$ M = g_x + g_y \\rightarrow \\frac{\\partial{M}}{\\partial{g_x}} = 1  $$ \n",
    "$$ \\frac{\\partial{M}}{\\partial{g_y}} = 1$$\n",
    "- #### loss -> square root\n",
    "\n",
    "$$ L = M^{\\frac{1}{2}} \\rightarrow \\frac{\\partial{L}}{\\partial{M}} = \\frac{1}{2} M^{\\frac{-1}{2}} $$\n",
    "\n",
    "\n",
    "- what we did was splitting down $L$ into smaller functions so in order to calculated the gradient at each step all we have to do is multiplying the local gradients\n",
    "$$ \\frac{\\partial{g_x}}{\\partial{x_p}}  =  \\frac{\\partial{g_x}}{\\partial{I_x}} * \\frac{\\partial{I_x}}{\\partial{x_p}} = 2 I_x * -1 $$\n",
    "\n",
    "$$ \\frac{\\partial{M}}{\\partial{x_p}}  =  \\frac{\\partial{g_x}}{\\partial{I_x}} * \\frac{\\partial{I_x}}{\\partial{x_p}} + \\frac{\\partial{g_y}}{\\partial{I_y}} * \\frac{\\partial{I_y}}{\\partial{x_p}}= 2 I_x * -1 + 0 $$\n",
    "\n",
    "- Total back-propagation using chain rule will be calculated as follows, \n",
    "- remember that's only wr to $x_p$, and duo to similarity we don't have to write the backprop steps for $y_p$\n",
    "- to save more steps, i will just use the calcuated grads for $\\frac{\\partial{M}}{\\partial{x_p}}$\n",
    "  \n",
    "$$ \\frac{\\partial{L}}{\\partial{x_p}} = \\frac{\\partial{L}}{\\partial{M}} * \\frac{\\partial{M}}{\\partial{x_p}} = \\frac{1}{2} M^{\\frac{-1}{2}} * -2 * I_x $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## explanation\n",
    "- Idea is to find a way to compute the gradients for any functions withiout having to do it manually\n",
    "- split any function into smaller functions\n",
    "- calculate the local gradients at each step\n",
    "- Apply the chain rule to compute from the start to the end\n",
    "  - but then how can we trace back the gradients to the start??\n",
    "  - this is where the graphs comes in\n",
    "  - we create a graph of the operations we did to compute the loss\n",
    "  - then we trace back the gradients from the end to the start in the graph\n",
    "  - we can use the graph to compute the gradients for any function\n",
    "- the process of calculating the gradients using the chain rule and the graph is called back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
